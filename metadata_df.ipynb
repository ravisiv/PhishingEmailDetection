{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "609736b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import email\n",
    "from email.parser import HeaderParser\n",
    "from urlextract import URLExtract\n",
    "import base64\n",
    "\n",
    "master_urls = \"/Users/ravis/Library/CloudStorage/OneDrive-SouthernMethodistUniversity/CapstoneA/Data/urlsinmails.csv\"\n",
    "master_headers = \"/Users/ravis/Library/CloudStorage/OneDrive-SouthernMethodistUniversity/CapstoneA/Data/headersinmails.csv\"\n",
    "master_topdomains = \"/Users/ravis/Library/CloudStorage/OneDrive-SouthernMethodistUniversity/CapstoneA/Data/topdomainsinmails.csv\"\n",
    "master_subdomains = \"/Users/ravis/Library/CloudStorage/OneDrive-SouthernMethodistUniversity/CapstoneA/Data/subdomainsinmails.csv\"\n",
    "master_suffix = \"/Users/ravis/Library/CloudStorage/OneDrive-SouthernMethodistUniversity/CapstoneA/Data/suffixinmails.csv\"\n",
    "\n",
    "\n",
    "urls_df = pd.read_csv(master_urls)\n",
    "headers_df = pd.read_csv(master_headers)\n",
    "\n",
    "#domain and subdomain must be a complex number\n",
    "\n",
    "topdomains_df = pd.read_csv(master_topdomains)\n",
    "subdomains_df = pd.read_csv(master_subdomains)\n",
    "suffix_df = pd.read_csv(master_suffix)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7cf69a14",
   "metadata": {},
   "outputs": [],
   "source": [
    "meta_urls = {}\n",
    "meta_headers = {}\n",
    "\n",
    "#TODO: This method need lots of tuning\n",
    "def get_email_as_dict(msg, urls_df,headers_df,topdomains_df,subdomains_df,suffix_df ):\n",
    "    \n",
    "    #untuned code\n",
    "    extractor = URLExtract()\n",
    "    parser = email.parser.HeaderParser()\n",
    "    headers = parser.parsestr(msg.as_string())\n",
    "\n",
    "    metadata = {}\n",
    "    all_urls = []\n",
    "    all_headers = []\n",
    "    \n",
    "    for h in headers.items():\n",
    "        urls = extractor.find_urls(h[1])\n",
    "        all_urls = all_urls + urls\n",
    "        #base64 decode\n",
    "        if isBase64(h[1]):\n",
    "            try:\n",
    "                b64_h1= base64.b64decode(h[1]).decode(\"utf-8\")\n",
    "                urls = extractor.find_urls(str(b64_h1))\n",
    "                all_urls = all_urls + urls\n",
    "\n",
    "            except Exception as e:\n",
    "                b64_h1 = \"\"\n",
    "        # email_headers[h[0]] = [h[1]]\n",
    "        all_headers.append(h[0])\n",
    "        \n",
    "    body = parse_body(msg)\n",
    "    attchments = get_attachments(msg)\n",
    "    str_body = str(body)\n",
    "\n",
    "    urls = extractor.find_urls(str(body))\n",
    "    \n",
    "    all_urls = all_urls + urls\n",
    "    all_urls_vectors = []\n",
    "    all_topdomains_vectors = []\n",
    "    all_subdomains_vectors = []\n",
    "    all_suffix_vectors = []\n",
    "    \n",
    "    all_headers_vectors = []\n",
    "    for each_url in all_urls:\n",
    "        strip_url = each_url.lower().strip().replace(\"\\\\n\",\"\").replace(\"\\\\t\",\"\").replace(\"nhttp\",\"http\")\n",
    "        sdomain,tdomain, suffix = get_domain(strip_url)\n",
    "        #print(\"sdomain = \",sdomain,\"tdomain = \",tdomain,\"suffix = \",suffix, \"url=\",strip_url)\n",
    "\n",
    "        try:\n",
    "            url_index = urls_df[\"url\"].to_list().index(strip_url) + 1\n",
    "            all_urls_vectors.append(url_index)\n",
    "        except ValueError as e:\n",
    "            urls_df = pd.concat([urls_df,pd.DataFrame({\"url\":[strip_url]})], ignore_index = True)\n",
    "            urls_df.reset_index()\n",
    "            url_index = urls_df[\"url\"].shape[0]\n",
    "            all_urls_vectors.append(url_index)\n",
    "\n",
    "        \n",
    "        if sdomain != '':\n",
    "            try:\n",
    "                subdomain_index = subdomains_df[\"subdomain\"].to_list().index(sdomain) + 1\n",
    "                all_subdomains_vectors.append(subdomain_index)\n",
    "            except ValueError as e:\n",
    "                subdomains_df = pd.concat([subdomains_df,pd.DataFrame({\"subdomain\":[sdomain]})], ignore_index = True)\n",
    "                subdomains_df.reset_index()\n",
    "                subdomain_index = subdomains_df[\"subdomain\"].shape[0]\n",
    "                all_subdomains_vectors.append(subdomain_index)\n",
    "        else:\n",
    "            all_subdomains_vectors.append(0)\n",
    "\n",
    "        try:\n",
    "            topdomain_index = topdomains_df[\"topdomain\"].to_list().index(tdomain) + 1\n",
    "            all_topdomains_vectors.append(topdomain_index)\n",
    "        except ValueError as e:\n",
    "            topdomains_df = pd.concat([topdomains_df,pd.DataFrame({\"topdomain\":[tdomain]})], ignore_index = True)\n",
    "            topdomains_df.reset_index()\n",
    "            topdomain_index = topdomains_df[\"topdomain\"].shape[0]\n",
    "            all_topdomains_vectors.append(topdomain_index)\n",
    "            \n",
    "        if suffix != '':\n",
    "            try:\n",
    "                suffix_index = suffix_df[\"suffix\"].to_list().index(suffix) + 1\n",
    "                all_suffix_vectors.append(suffix_index)\n",
    "            except ValueError as e:\n",
    "                suffix_df = pd.concat([suffix_df,pd.DataFrame({\"suffix\":[suffix]})], ignore_index = True)\n",
    "                suffix_df.reset_index()\n",
    "                suffix_index = suffix_df[\"suffix\"].shape[0]\n",
    "                all_suffix_vectors.append(suffix_index)\n",
    "        else:\n",
    "            all_suffix_vectors.append(0)\n",
    "\n",
    "\n",
    "            \n",
    "    for each_header in all_headers:\n",
    "        strip_header = each_header.lower().strip()\n",
    "        try:\n",
    "            header_index = headers_df[\"header\"].to_list().index(strip_header) + 1\n",
    "            all_headers_vectors.append(header_index)\n",
    "\n",
    "        except ValueError as e:\n",
    "            headers_df = pd.concat([headers_df,pd.DataFrame({\"header\":[strip_header]})], ignore_index = True)\n",
    "            headers_df.reset_index()\n",
    "            header_index = headers_df[\"header\"].shape[0]\n",
    "            all_headers_vectors.append(header_index)\n",
    "        \n",
    "    dict_row_urls = get_as_row(metatype=\"url\",list_data=all_urls_vectors)\n",
    "    dict_row_header = get_as_row(metatype=\"header\",list_data=all_headers_vectors)\n",
    "    dict_row_topdomain = get_as_row(metatype=\"topdomain\",list_data=all_topdomains_vectors)\n",
    "    dict_row_subdomain = get_as_row(metatype=\"subdomain\",list_data=all_subdomains_vectors)\n",
    "    dict_row_suffix = get_as_row(metatype=\"suffix\",list_data=all_suffix_vectors)\n",
    "    \n",
    "    metadata.update(dict_row_urls) \n",
    "    metadata.update(dict_row_header)\n",
    "    metadata.update(dict_row_topdomain)\n",
    "    metadata.update(dict_row_subdomain)\n",
    "    metadata.update(dict_row_suffix)\n",
    "    metadata.update(attchments)\n",
    "    \n",
    "    return metadata, str_body,  urls_df,headers_df,topdomains_df,subdomains_df,suffix_df \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "59d1e079",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_email_as_df(msg, urls_df,headers_df,topdomains_df,subdomains_df,suffix_df ):\n",
    "    metadata, body,urls_df,headers_df,topdomains_df,subdomains_df,suffix_df  = get_email_as_dict(msg, urls_df,headers_df,topdomains_df,subdomains_df,suffix_df )\n",
    "    metadata_df = pd.DataFrame.from_dict([metadata])\n",
    "    return metadata_df,body,urls_df,headers_df,topdomains_df,subdomains_df,suffix_df \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6bc5267f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def isBase64(sb):\n",
    "    try:\n",
    "        if isinstance(sb, str):\n",
    "                # If there's any unicode here, an exception will be thrown and the function will return false\n",
    "                sb_bytes = bytes(sb, 'ascii')\n",
    "        elif isinstance(sb, bytes):\n",
    "                sb_bytes = sb\n",
    "        else:\n",
    "                raise ValueError(\"Argument must be string or bytes\")\n",
    "        return base64.b64encode(base64.b64decode(sb_bytes)) == sb_bytes\n",
    "    except Exception:\n",
    "                return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6d979fe1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tldextract\n",
    "\n",
    "def get_domain(urlwithdomain):\n",
    "    res = tldextract.extract(urlwithdomain)\n",
    "    if res[2] == '':\n",
    "        return res[0], res[1] , res[2]\n",
    "    else:\n",
    "        return res[0], res[1] + \".\" + res[2], res[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "686663fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_as_row(metatype,list_data):\n",
    "    row = {}\n",
    "    for index,eachitem in enumerate(list_data):\n",
    "        row[metatype + str(index+1)] = eachitem\n",
    "    return row"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "cd85c8b9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('', 'google.com', 'com')"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_domain(\"http://google.com\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "12ccfa03",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reference: \n",
    "# https://stackoverflow.com/questions/17874360/python-how-to-parse-the-body-from-a-raw-email-given-that-raw-email-does-not\n",
    "#body = \"\"\n",
    "def parse_body(email_msg):\n",
    "    bodies = []  \n",
    "    for part in email_msg.walk():\n",
    "        ctype = part.get_content_type()\n",
    "        cdispo = str(part.get('Content-Disposition'))\n",
    "        if ctype == 'text/plain' and 'attachment' not in cdispo:\n",
    "            bodies.append(part.get_payload(decode=True))\n",
    "        else:\n",
    "            bodies.append(email_msg.get_payload(decode=True))\n",
    "        \n",
    "    return bodies[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "5644a781",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Reference https://stackoverflow.com/questions/1936466/beautifulsoup-grab-visible-webpage-text\n",
    "\n",
    "from bs4 import BeautifulSoup\n",
    "from bs4.element import Comment\n",
    "\n",
    "def tag_visible(element):\n",
    "    if element.parent.name in ['style', 'script', 'head', 'title', 'meta', '[document]']:\n",
    "        return False\n",
    "    if isinstance(element, Comment):\n",
    "        return False\n",
    "    return True\n",
    "\n",
    "\n",
    "def text_from_html(body):\n",
    "    soup = BeautifulSoup(body, 'html.parser')\n",
    "    texts = soup.findAll(text=True)\n",
    "    visible_texts = filter(tag_visible, texts)  \n",
    "    return u\" \".join(t.strip() for t in visible_texts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e3c85bb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import entropy\n",
    "import pickle\n",
    "\n",
    "def get_attachments(email_msg):\n",
    "    attachments={}\n",
    "    if email_msg == None:\n",
    "        return {}\n",
    "    attachment_count = 0\n",
    "    for part in email_msg.walk():\n",
    "        content_dis = part.get_content_disposition()\n",
    "        if content_dis == \"attachment\":\n",
    "            att_filename = part.get_filename()\n",
    "            payload = bytearray(part.get_payload(decode=True))\n",
    "            att_entropy= entropy(payload,base=2)\n",
    "            att_size = len(payload)\n",
    "            prefix = \"attachment\" + str(attachment_count)\n",
    "            attachment = {}\n",
    "            attachment[prefix + \"_filename\"] = att_filename\n",
    "            attachment[prefix + \"_entropy\"] = att_entropy\n",
    "            attachment[prefix + \"_size\"] = att_size\n",
    "            #attachments.update(attachment)\n",
    "            attachment_count+=1\n",
    "    attachment_count = {}\n",
    "    attachment_count[\"attachment_count\"] = len(attachments)//3\n",
    "    attachments.update(attachment_count)\n",
    "    return attachments\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "ed53fb9e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------starting------------\n",
      "Traceback (most recent call last):\n",
      "  File \"/var/folders/rr/gnmws3gs2xn3p2l0t4_lq7hc0000gn/T/ipykernel_77245/3341638525.py\", line 17, in <cell line: 9>\n",
      "    email_df, body,urls_df,headers_df,topdomains_df,subdomains_df,suffix_df  = get_email_as_df(x, urls_df,headers_df,topdomains_df,subdomains_df,suffix_df )\n",
      "  File \"/var/folders/rr/gnmws3gs2xn3p2l0t4_lq7hc0000gn/T/ipykernel_77245/1963689943.py\", line 2, in get_email_as_df\n",
      "    metadata, body,urls_df,headers_df,topdomains_df,subdomains_df,suffix_df  = get_email_as_dict(msg, urls_df,headers_df,topdomains_df,subdomains_df,suffix_df )\n",
      "  File \"/var/folders/rr/gnmws3gs2xn3p2l0t4_lq7hc0000gn/T/ipykernel_77245/3848541455.py\", line 17, in get_email_as_dict\n",
      "    urls = extractor.find_urls(h[1])\n",
      "  File \"/Users/ravis/miniforge3/envs/tensorflow/lib/python3.9/site-packages/urlextract/urlextract_core.py\", line 815, in find_urls\n",
      "    url = next(urls, \"\")\n",
      "  File \"/Users/ravis/miniforge3/envs/tensorflow/lib/python3.9/site-packages/urlextract/urlextract_core.py\", line 742, in gen_urls\n",
      "    tmp_url = self._complete_url(\n",
      "  File \"/Users/ravis/miniforge3/envs/tensorflow/lib/python3.9/site-packages/urlextract/urlextract_core.py\", line 456, in _complete_url\n",
      "    if not self._is_domain_valid(\n",
      "  File \"/Users/ravis/miniforge3/envs/tensorflow/lib/python3.9/site-packages/urlextract/urlextract_core.py\", line 542, in _is_domain_valid\n",
      "    if url_parts.getuserinfo() and added_schema:\n",
      "  File \"/Users/ravis/miniforge3/envs/tensorflow/lib/python3.9/site-packages/uritools/__init__.py\", line 239, in getuserinfo\n",
      "    return uridecode(userinfo, encoding, errors)\n",
      "  File \"/Users/ravis/miniforge3/envs/tensorflow/lib/python3.9/site-packages/uritools/__init__.py\", line 104, in uridecode\n",
      "    return b\"\".join(result).decode(encoding, errors)\n",
      "UnicodeDecodeError: 'utf-8' codec can't decode byte 0xda in position 15: invalid continuation byte\n",
      "\n",
      "--------------------------------ending------------\n",
      "--------------------------------starting------------\n",
      "Traceback (most recent call last):\n",
      "  File \"/var/folders/rr/gnmws3gs2xn3p2l0t4_lq7hc0000gn/T/ipykernel_77245/3341638525.py\", line 17, in <cell line: 9>\n",
      "    email_df, body,urls_df,headers_df,topdomains_df,subdomains_df,suffix_df  = get_email_as_df(x, urls_df,headers_df,topdomains_df,subdomains_df,suffix_df )\n",
      "  File \"/var/folders/rr/gnmws3gs2xn3p2l0t4_lq7hc0000gn/T/ipykernel_77245/1963689943.py\", line 2, in get_email_as_df\n",
      "    metadata, body,urls_df,headers_df,topdomains_df,subdomains_df,suffix_df  = get_email_as_dict(msg, urls_df,headers_df,topdomains_df,subdomains_df,suffix_df )\n",
      "  File \"/var/folders/rr/gnmws3gs2xn3p2l0t4_lq7hc0000gn/T/ipykernel_77245/3848541455.py\", line 17, in get_email_as_dict\n",
      "    urls = extractor.find_urls(h[1])\n",
      "  File \"/Users/ravis/miniforge3/envs/tensorflow/lib/python3.9/site-packages/urlextract/urlextract_core.py\", line 815, in find_urls\n",
      "    url = next(urls, \"\")\n",
      "  File \"/Users/ravis/miniforge3/envs/tensorflow/lib/python3.9/site-packages/urlextract/urlextract_core.py\", line 742, in gen_urls\n",
      "    tmp_url = self._complete_url(\n",
      "  File \"/Users/ravis/miniforge3/envs/tensorflow/lib/python3.9/site-packages/urlextract/urlextract_core.py\", line 456, in _complete_url\n",
      "    if not self._is_domain_valid(\n",
      "  File \"/Users/ravis/miniforge3/envs/tensorflow/lib/python3.9/site-packages/urlextract/urlextract_core.py\", line 542, in _is_domain_valid\n",
      "    if url_parts.getuserinfo() and added_schema:\n",
      "  File \"/Users/ravis/miniforge3/envs/tensorflow/lib/python3.9/site-packages/uritools/__init__.py\", line 239, in getuserinfo\n",
      "    return uridecode(userinfo, encoding, errors)\n",
      "  File \"/Users/ravis/miniforge3/envs/tensorflow/lib/python3.9/site-packages/uritools/__init__.py\", line 104, in uridecode\n",
      "    return b\"\".join(result).decode(encoding, errors)\n",
      "UnicodeDecodeError: 'utf-8' codec can't decode byte 0xda in position 15: invalid continuation byte\n",
      "\n",
      "--------------------------------ending------------\n",
      "--------------------------------starting------------\n",
      "Traceback (most recent call last):\n",
      "  File \"/var/folders/rr/gnmws3gs2xn3p2l0t4_lq7hc0000gn/T/ipykernel_77245/3341638525.py\", line 17, in <cell line: 9>\n",
      "    email_df, body,urls_df,headers_df,topdomains_df,subdomains_df,suffix_df  = get_email_as_df(x, urls_df,headers_df,topdomains_df,subdomains_df,suffix_df )\n",
      "  File \"/var/folders/rr/gnmws3gs2xn3p2l0t4_lq7hc0000gn/T/ipykernel_77245/1963689943.py\", line 2, in get_email_as_df\n",
      "    metadata, body,urls_df,headers_df,topdomains_df,subdomains_df,suffix_df  = get_email_as_dict(msg, urls_df,headers_df,topdomains_df,subdomains_df,suffix_df )\n",
      "  File \"/var/folders/rr/gnmws3gs2xn3p2l0t4_lq7hc0000gn/T/ipykernel_77245/3848541455.py\", line 17, in get_email_as_dict\n",
      "    urls = extractor.find_urls(h[1])\n",
      "  File \"/Users/ravis/miniforge3/envs/tensorflow/lib/python3.9/site-packages/urlextract/urlextract_core.py\", line 815, in find_urls\n",
      "    url = next(urls, \"\")\n",
      "  File \"/Users/ravis/miniforge3/envs/tensorflow/lib/python3.9/site-packages/urlextract/urlextract_core.py\", line 742, in gen_urls\n",
      "    tmp_url = self._complete_url(\n",
      "  File \"/Users/ravis/miniforge3/envs/tensorflow/lib/python3.9/site-packages/urlextract/urlextract_core.py\", line 456, in _complete_url\n",
      "    if not self._is_domain_valid(\n",
      "  File \"/Users/ravis/miniforge3/envs/tensorflow/lib/python3.9/site-packages/urlextract/urlextract_core.py\", line 542, in _is_domain_valid\n",
      "    if url_parts.getuserinfo() and added_schema:\n",
      "  File \"/Users/ravis/miniforge3/envs/tensorflow/lib/python3.9/site-packages/uritools/__init__.py\", line 239, in getuserinfo\n",
      "    return uridecode(userinfo, encoding, errors)\n",
      "  File \"/Users/ravis/miniforge3/envs/tensorflow/lib/python3.9/site-packages/uritools/__init__.py\", line 104, in uridecode\n",
      "    return b\"\".join(result).decode(encoding, errors)\n",
      "UnicodeDecodeError: 'utf-8' codec can't decode byte 0xda in position 15: invalid continuation byte\n",
      "\n",
      "--------------------------------ending------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/ravis/miniforge3/envs/tensorflow/lib/python3.9/site-packages/bs4/__init__.py:431: MarkupResemblesLocatorWarning: \"http://www.post-gazette.com/columnists/20020905brian5.asp\\n\\n\\n\\n\" looks like a URL. Beautiful Soup is not an HTTP client. You should probably use an HTTP client like requests to get the document behind the URL, and feed that document to Beautiful Soup.\n",
      "  warnings.warn(\n",
      "/Users/ravis/miniforge3/envs/tensorflow/lib/python3.9/site-packages/bs4/__init__.py:431: MarkupResemblesLocatorWarning: \"http://www.post-gazette.com/columnists/20020905brian5.asp\\n\\n\\n\\n\" looks like a URL. Beautiful Soup is not an HTTP client. You should probably use an HTTP client like requests to get the document behind the URL, and feed that document to Beautiful Soup.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------starting------------\n",
      "Traceback (most recent call last):\n",
      "  File \"/var/folders/rr/gnmws3gs2xn3p2l0t4_lq7hc0000gn/T/ipykernel_77245/3341638525.py\", line 17, in <cell line: 9>\n",
      "    email_df, body,urls_df,headers_df,topdomains_df,subdomains_df,suffix_df  = get_email_as_df(x, urls_df,headers_df,topdomains_df,subdomains_df,suffix_df )\n",
      "  File \"/var/folders/rr/gnmws3gs2xn3p2l0t4_lq7hc0000gn/T/ipykernel_77245/1963689943.py\", line 2, in get_email_as_df\n",
      "    metadata, body,urls_df,headers_df,topdomains_df,subdomains_df,suffix_df  = get_email_as_dict(msg, urls_df,headers_df,topdomains_df,subdomains_df,suffix_df )\n",
      "  File \"/var/folders/rr/gnmws3gs2xn3p2l0t4_lq7hc0000gn/T/ipykernel_77245/3848541455.py\", line 32, in get_email_as_dict\n",
      "    attchments = get_attachments(msg)\n",
      "  File \"/var/folders/rr/gnmws3gs2xn3p2l0t4_lq7hc0000gn/T/ipykernel_77245/2683923813.py\", line 13, in get_attachments\n",
      "    payload = bytearray(part.get_payload(decode=True))\n",
      "TypeError: cannot convert 'NoneType' object to bytearray\n",
      "\n",
      "--------------------------------ending------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/rr/gnmws3gs2xn3p2l0t4_lq7hc0000gn/T/ipykernel_77245/3341638525.py:34: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  metadata_df[\"target\"] = body_df[\"target\"]\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import traceback\n",
    "\n",
    "data_path = \"/Users/ravis/Library/CloudStorage/OneDrive-SouthernMethodistUniversity/CapstoneA/Data/phishingdata\"\n",
    "#data_path = \"/Users/ravis/Downloads/apruvspam\"\n",
    "sub_folders = [x[0] for x in os.walk(data_path) if x[0] != data_path]\n",
    "metadata_df = None\n",
    "body_list = []\n",
    "for folder in sub_folders:\n",
    "    files = [f for f in os.listdir(folder) if os.path.isfile(os.path.join(folder, f))]\n",
    "    for file in files:\n",
    "        with open(f\"{folder}/{file}\", encoding=\"latin1\") as f:\n",
    "            f_realpath = os.path.realpath(f.name)\n",
    "            is_spam = \"spam\" in f_realpath\n",
    "            x = email.message_from_file(f)\n",
    "            try:\n",
    "                email_df, body,urls_df,headers_df,topdomains_df,subdomains_df,suffix_df  = get_email_as_df(x, urls_df,headers_df,topdomains_df,subdomains_df,suffix_df )\n",
    "\n",
    "                attachments = get_attachments(x)\n",
    "                body_text = text_from_html(body[2:-1]).replace(\"\\\\n\",\"\").replace(\"\\\\t\",\"\")\n",
    "                body_list.append([body_text, attachments['attachment_count'],1 if is_spam is True else 0])\n",
    "                if metadata_df is None:\n",
    "                    metadata_df = email_df\n",
    "                else:\n",
    "                    metadata_df = pd.concat([metadata_df,email_df], ignore_index=True)\n",
    "            except Exception as e:\n",
    "                print(\"--------------------------------starting------------\")\n",
    "                print(traceback.format_exc())\n",
    "                print(\"--------------------------------ending------------\")\n",
    "\n",
    "                continue\n",
    "                \n",
    "body_df = pd.DataFrame(body_list, columns = [\"body\", \"attachment_count\",\"target\"]) \n",
    "metadata_df[\"target\"] = body_df[\"target\"]\n",
    "metadata_df[\"attachment_count\"] = body_df[\"attachment_count\"]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "98129cfd",
   "metadata": {},
   "outputs": [],
   "source": [
    "metadata_df = metadata_df.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "dfee335b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>attachment_count</th>\n",
       "      <th>url1</th>\n",
       "      <th>url2</th>\n",
       "      <th>url3</th>\n",
       "      <th>url4</th>\n",
       "      <th>url5</th>\n",
       "      <th>url6</th>\n",
       "      <th>url7</th>\n",
       "      <th>url8</th>\n",
       "      <th>url9</th>\n",
       "      <th>...</th>\n",
       "      <th>suffix246</th>\n",
       "      <th>suffix247</th>\n",
       "      <th>suffix248</th>\n",
       "      <th>suffix249</th>\n",
       "      <th>suffix250</th>\n",
       "      <th>suffix251</th>\n",
       "      <th>suffix252</th>\n",
       "      <th>suffix253</th>\n",
       "      <th>suffix254</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>29213.0</td>\n",
       "      <td>7580.0</td>\n",
       "      <td>29214.0</td>\n",
       "      <td>29215.0</td>\n",
       "      <td>29216.0</td>\n",
       "      <td>29214.0</td>\n",
       "      <td>21061.0</td>\n",
       "      <td>7580.0</td>\n",
       "      <td>29214.0</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>7580.0</td>\n",
       "      <td>29236.0</td>\n",
       "      <td>29237.0</td>\n",
       "      <td>29236.0</td>\n",
       "      <td>21061.0</td>\n",
       "      <td>7580.0</td>\n",
       "      <td>29236.0</td>\n",
       "      <td>29236.0</td>\n",
       "      <td>7580.0</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>5600.0</td>\n",
       "      <td>29238.0</td>\n",
       "      <td>7580.0</td>\n",
       "      <td>29239.0</td>\n",
       "      <td>29240.0</td>\n",
       "      <td>29241.0</td>\n",
       "      <td>29241.0</td>\n",
       "      <td>29239.0</td>\n",
       "      <td>21061.0</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9964</th>\n",
       "      <td>0</td>\n",
       "      <td>20245.0</td>\n",
       "      <td>343.0</td>\n",
       "      <td>21874.0</td>\n",
       "      <td>343.0</td>\n",
       "      <td>23203.0</td>\n",
       "      <td>20603.0</td>\n",
       "      <td>1292.0</td>\n",
       "      <td>7063.0</td>\n",
       "      <td>23042.0</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9965</th>\n",
       "      <td>0</td>\n",
       "      <td>20245.0</td>\n",
       "      <td>343.0</td>\n",
       "      <td>21874.0</td>\n",
       "      <td>343.0</td>\n",
       "      <td>23481.0</td>\n",
       "      <td>4693.0</td>\n",
       "      <td>7063.0</td>\n",
       "      <td>20154.0</td>\n",
       "      <td>20245.0</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9966</th>\n",
       "      <td>0</td>\n",
       "      <td>20245.0</td>\n",
       "      <td>343.0</td>\n",
       "      <td>21874.0</td>\n",
       "      <td>7063.0</td>\n",
       "      <td>3244.0</td>\n",
       "      <td>23203.0</td>\n",
       "      <td>20603.0</td>\n",
       "      <td>1292.0</td>\n",
       "      <td>7063.0</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9967</th>\n",
       "      <td>0</td>\n",
       "      <td>20245.0</td>\n",
       "      <td>343.0</td>\n",
       "      <td>21874.0</td>\n",
       "      <td>343.0</td>\n",
       "      <td>7181.0</td>\n",
       "      <td>21505.0</td>\n",
       "      <td>1304.0</td>\n",
       "      <td>7063.0</td>\n",
       "      <td>6464.0</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9968</th>\n",
       "      <td>0</td>\n",
       "      <td>20245.0</td>\n",
       "      <td>343.0</td>\n",
       "      <td>21874.0</td>\n",
       "      <td>343.0</td>\n",
       "      <td>20223.0</td>\n",
       "      <td>20223.0</td>\n",
       "      <td>5157.0</td>\n",
       "      <td>7063.0</td>\n",
       "      <td>20223.0</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>9969 rows × 1143 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      attachment_count     url1     url2     url3     url4     url5     url6  \\\n",
       "0                    0      NaN      NaN      NaN      NaN      NaN      NaN   \n",
       "1                    0      NaN      NaN      NaN      NaN      NaN      NaN   \n",
       "2                    0  29213.0   7580.0  29214.0  29215.0  29216.0  29214.0   \n",
       "3                    0   7580.0  29236.0  29237.0  29236.0  21061.0   7580.0   \n",
       "4                    0   5600.0  29238.0   7580.0  29239.0  29240.0  29241.0   \n",
       "...                ...      ...      ...      ...      ...      ...      ...   \n",
       "9964                 0  20245.0    343.0  21874.0    343.0  23203.0  20603.0   \n",
       "9965                 0  20245.0    343.0  21874.0    343.0  23481.0   4693.0   \n",
       "9966                 0  20245.0    343.0  21874.0   7063.0   3244.0  23203.0   \n",
       "9967                 0  20245.0    343.0  21874.0    343.0   7181.0  21505.0   \n",
       "9968                 0  20245.0    343.0  21874.0    343.0  20223.0  20223.0   \n",
       "\n",
       "         url7     url8     url9  ...  suffix246  suffix247  suffix248  \\\n",
       "0         NaN      NaN      NaN  ...        NaN        NaN        NaN   \n",
       "1         NaN      NaN      NaN  ...        NaN        NaN        NaN   \n",
       "2     21061.0   7580.0  29214.0  ...        NaN        NaN        NaN   \n",
       "3     29236.0  29236.0   7580.0  ...        NaN        NaN        NaN   \n",
       "4     29241.0  29239.0  21061.0  ...        NaN        NaN        NaN   \n",
       "...       ...      ...      ...  ...        ...        ...        ...   \n",
       "9964   1292.0   7063.0  23042.0  ...        NaN        NaN        NaN   \n",
       "9965   7063.0  20154.0  20245.0  ...        NaN        NaN        NaN   \n",
       "9966  20603.0   1292.0   7063.0  ...        NaN        NaN        NaN   \n",
       "9967   1304.0   7063.0   6464.0  ...        NaN        NaN        NaN   \n",
       "9968   5157.0   7063.0  20223.0  ...        NaN        NaN        NaN   \n",
       "\n",
       "      suffix249  suffix250  suffix251  suffix252  suffix253  suffix254  target  \n",
       "0           NaN        NaN        NaN        NaN        NaN        NaN       0  \n",
       "1           NaN        NaN        NaN        NaN        NaN        NaN       1  \n",
       "2           NaN        NaN        NaN        NaN        NaN        NaN       1  \n",
       "3           NaN        NaN        NaN        NaN        NaN        NaN       1  \n",
       "4           NaN        NaN        NaN        NaN        NaN        NaN       1  \n",
       "...         ...        ...        ...        ...        ...        ...     ...  \n",
       "9964        NaN        NaN        NaN        NaN        NaN        NaN       0  \n",
       "9965        NaN        NaN        NaN        NaN        NaN        NaN       0  \n",
       "9966        NaN        NaN        NaN        NaN        NaN        NaN       0  \n",
       "9967        NaN        NaN        NaN        NaN        NaN        NaN       0  \n",
       "9968        NaN        NaN        NaN        NaN        NaN        NaN       0  \n",
       "\n",
       "[9969 rows x 1143 columns]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "metadata_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35aea1cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(metadata_df.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "209d25ce",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "body_df.to_csv(\"~/Downloads/bodytext.csv\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbd705a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "mytext = \"this contains ham\"\n",
    "\n",
    "is_spam = \"spam\" in mytext\n",
    "\n",
    "print(1 if is_spam is True else 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78e385e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(get_domain('http://hello.google.com'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1feada73",
   "metadata": {},
   "outputs": [],
   "source": [
    "urls_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5f914d9",
   "metadata": {},
   "source": [
    "### Extract Domains from URLs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb6f6f4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "urls_df = pd.read_csv(master_urls)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a7b4a4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "subdomains = []\n",
    "topdomains = []\n",
    "suffixes = []\n",
    "for index, row in urls_df.iterrows():\n",
    "    url_data = row[\"url\"]\n",
    "    if url_data.startswith(\"http\") == False:\n",
    "        url_data = \"http://\" + url_data\n",
    "    sub_domain, top_domain, suffix = get_domain(url_data)\n",
    "    subdomains.append(sub_domain)\n",
    "    topdomains.append(top_domain)\n",
    "    \n",
    "urls_df[\"subdomain\"] = subdomains\n",
    "urls_df[\"domain\"] = topdomains\n",
    "urls_df.to_csv(\"/Users/ravis/Library/CloudStorage/OneDrive-SouthernMethodistUniversity/CapstoneA/Code/extract_url_data/data/urls.csv\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15eb5f1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "sub_df = pd.DataFrame(subdomains_uniq)\n",
    "top_df = pd.DataFrame(topdomains_uniq)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff0e8db1",
   "metadata": {},
   "outputs": [],
   "source": [
    "sub_df.to_csv(\"/Users/ravis/Library/CloudStorage/OneDrive-SouthernMethodistUniversity/CapstoneA/Data/subdomains.csv\")\n",
    "top_df.to_csv(\"/Users/ravis/Library/CloudStorage/OneDrive-SouthernMethodistUniversity/CapstoneA/Data/topdomains.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "2fb593b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "metadata_df.to_csv(\"/Users/ravis/Library/CloudStorage/OneDrive-SouthernMethodistUniversity/CapstoneA/Data/FinalData/metadata.csv\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "f89bc420",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>url1</th>\n",
       "      <th>url2</th>\n",
       "      <th>url3</th>\n",
       "      <th>url4</th>\n",
       "      <th>url5</th>\n",
       "      <th>url6</th>\n",
       "      <th>url7</th>\n",
       "      <th>url8</th>\n",
       "      <th>url9</th>\n",
       "      <th>url10</th>\n",
       "      <th>...</th>\n",
       "      <th>header116</th>\n",
       "      <th>header117</th>\n",
       "      <th>header118</th>\n",
       "      <th>header119</th>\n",
       "      <th>header120</th>\n",
       "      <th>header121</th>\n",
       "      <th>header122</th>\n",
       "      <th>header123</th>\n",
       "      <th>header124</th>\n",
       "      <th>header125</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>343.0</td>\n",
       "      <td>23547.0</td>\n",
       "      <td>343.0</td>\n",
       "      <td>22463.0</td>\n",
       "      <td>22463.0</td>\n",
       "      <td>3454.0</td>\n",
       "      <td>7063.0</td>\n",
       "      <td>20818.0</td>\n",
       "      <td>20767.0</td>\n",
       "      <td>3453.0</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>207.0</td>\n",
       "      <td>343.0</td>\n",
       "      <td>23547.0</td>\n",
       "      <td>343.0</td>\n",
       "      <td>23203.0</td>\n",
       "      <td>20603.0</td>\n",
       "      <td>1292.0</td>\n",
       "      <td>7063.0</td>\n",
       "      <td>22619.0</td>\n",
       "      <td>22619.0</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>343.0</td>\n",
       "      <td>23547.0</td>\n",
       "      <td>343.0</td>\n",
       "      <td>23203.0</td>\n",
       "      <td>20603.0</td>\n",
       "      <td>1292.0</td>\n",
       "      <td>7063.0</td>\n",
       "      <td>20640.0</td>\n",
       "      <td>7790.0</td>\n",
       "      <td>5000.0</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>343.0</td>\n",
       "      <td>23547.0</td>\n",
       "      <td>343.0</td>\n",
       "      <td>23203.0</td>\n",
       "      <td>20603.0</td>\n",
       "      <td>1292.0</td>\n",
       "      <td>7063.0</td>\n",
       "      <td>23549.0</td>\n",
       "      <td>22823.0</td>\n",
       "      <td>5057.0</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>343.0</td>\n",
       "      <td>23547.0</td>\n",
       "      <td>343.0</td>\n",
       "      <td>23203.0</td>\n",
       "      <td>20603.0</td>\n",
       "      <td>1292.0</td>\n",
       "      <td>7063.0</td>\n",
       "      <td>21990.0</td>\n",
       "      <td>1489.0</td>\n",
       "      <td>23203.0</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5618</th>\n",
       "      <td>343.0</td>\n",
       "      <td>20046.0</td>\n",
       "      <td>343.0</td>\n",
       "      <td>23481.0</td>\n",
       "      <td>4693.0</td>\n",
       "      <td>7063.0</td>\n",
       "      <td>20154.0</td>\n",
       "      <td>20245.0</td>\n",
       "      <td>343.0</td>\n",
       "      <td>23481.0</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5619</th>\n",
       "      <td>20245.0</td>\n",
       "      <td>343.0</td>\n",
       "      <td>21874.0</td>\n",
       "      <td>343.0</td>\n",
       "      <td>23481.0</td>\n",
       "      <td>4693.0</td>\n",
       "      <td>7063.0</td>\n",
       "      <td>20154.0</td>\n",
       "      <td>20245.0</td>\n",
       "      <td>343.0</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5620</th>\n",
       "      <td>343.0</td>\n",
       "      <td>20046.0</td>\n",
       "      <td>343.0</td>\n",
       "      <td>7063.0</td>\n",
       "      <td>20245.0</td>\n",
       "      <td>343.0</td>\n",
       "      <td>28193.0</td>\n",
       "      <td>17517.0</td>\n",
       "      <td>17519.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5621</th>\n",
       "      <td>343.0</td>\n",
       "      <td>20046.0</td>\n",
       "      <td>343.0</td>\n",
       "      <td>6464.0</td>\n",
       "      <td>6464.0</td>\n",
       "      <td>1304.0</td>\n",
       "      <td>7063.0</td>\n",
       "      <td>6464.0</td>\n",
       "      <td>20245.0</td>\n",
       "      <td>343.0</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5622</th>\n",
       "      <td>20245.0</td>\n",
       "      <td>343.0</td>\n",
       "      <td>21874.0</td>\n",
       "      <td>343.0</td>\n",
       "      <td>23481.0</td>\n",
       "      <td>4693.0</td>\n",
       "      <td>7063.0</td>\n",
       "      <td>20154.0</td>\n",
       "      <td>20245.0</td>\n",
       "      <td>343.0</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5623 rows × 870 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         url1     url2     url3     url4     url5     url6     url7     url8  \\\n",
       "0       343.0  23547.0    343.0  22463.0  22463.0   3454.0   7063.0  20818.0   \n",
       "1       207.0    343.0  23547.0    343.0  23203.0  20603.0   1292.0   7063.0   \n",
       "2       343.0  23547.0    343.0  23203.0  20603.0   1292.0   7063.0  20640.0   \n",
       "3       343.0  23547.0    343.0  23203.0  20603.0   1292.0   7063.0  23549.0   \n",
       "4       343.0  23547.0    343.0  23203.0  20603.0   1292.0   7063.0  21990.0   \n",
       "...       ...      ...      ...      ...      ...      ...      ...      ...   \n",
       "5618    343.0  20046.0    343.0  23481.0   4693.0   7063.0  20154.0  20245.0   \n",
       "5619  20245.0    343.0  21874.0    343.0  23481.0   4693.0   7063.0  20154.0   \n",
       "5620    343.0  20046.0    343.0   7063.0  20245.0    343.0  28193.0  17517.0   \n",
       "5621    343.0  20046.0    343.0   6464.0   6464.0   1304.0   7063.0   6464.0   \n",
       "5622  20245.0    343.0  21874.0    343.0  23481.0   4693.0   7063.0  20154.0   \n",
       "\n",
       "         url9    url10  ...  header116  header117  header118  header119  \\\n",
       "0     20767.0   3453.0  ...        NaN        NaN        NaN        NaN   \n",
       "1     22619.0  22619.0  ...        NaN        NaN        NaN        NaN   \n",
       "2      7790.0   5000.0  ...        NaN        NaN        NaN        NaN   \n",
       "3     22823.0   5057.0  ...        NaN        NaN        NaN        NaN   \n",
       "4      1489.0  23203.0  ...        NaN        NaN        NaN        NaN   \n",
       "...       ...      ...  ...        ...        ...        ...        ...   \n",
       "5618    343.0  23481.0  ...        NaN        NaN        NaN        NaN   \n",
       "5619  20245.0    343.0  ...        NaN        NaN        NaN        NaN   \n",
       "5620  17519.0      NaN  ...        NaN        NaN        NaN        NaN   \n",
       "5621  20245.0    343.0  ...        NaN        NaN        NaN        NaN   \n",
       "5622  20245.0    343.0  ...        NaN        NaN        NaN        NaN   \n",
       "\n",
       "      header120  header121  header122  header123  header124  header125  \n",
       "0           NaN        NaN        NaN        NaN        NaN        NaN  \n",
       "1           NaN        NaN        NaN        NaN        NaN        NaN  \n",
       "2           NaN        NaN        NaN        NaN        NaN        NaN  \n",
       "3           NaN        NaN        NaN        NaN        NaN        NaN  \n",
       "4           NaN        NaN        NaN        NaN        NaN        NaN  \n",
       "...         ...        ...        ...        ...        ...        ...  \n",
       "5618        NaN        NaN        NaN        NaN        NaN        NaN  \n",
       "5619        NaN        NaN        NaN        NaN        NaN        NaN  \n",
       "5620        NaN        NaN        NaN        NaN        NaN        NaN  \n",
       "5621        NaN        NaN        NaN        NaN        NaN        NaN  \n",
       "5622        NaN        NaN        NaN        NaN        NaN        NaN  \n",
       "\n",
       "[5623 rows x 870 columns]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "metadata_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8bf2b6c9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
